{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112635a0",
   "metadata": {},
   "source": [
    "# 임베딩\n",
    "\n",
    "임베딩의 역할\n",
    "- 단어 및 문장 간 관련성 계산\n",
    "- 의미적 혹은 문법적 정보의 함축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2478b",
   "metadata": {},
   "source": [
    "# 희소표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd14025c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원 핫 인코딩 적용\n",
    "import pandas as pd\n",
    "class2=pd.read_csv(\"C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/class2.csv\")\n",
    "\n",
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder() # 데이터를 숫자형식으로 표현\n",
    "\n",
    "train_x = label_encoder.fit_transform(class2['class2'])\n",
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9eb07",
   "metadata": {},
   "source": [
    "# 횟수기반 임베딩\n",
    "\n",
    "- 단어가 출현한 빈도를 고려햐여 이용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4064bf",
   "metadata": {},
   "source": [
    "## 카운터 벡터\n",
    "\n",
    "문서 집합에서 단어를 토큰으로 생성하고 각 단어의 출현 빈도수를 이용하여 인코딩해 벡터로 만드는 방법 즉 토크나이징과 벡터화가 동시에 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b54248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 13,\n",
       " 'is': 7,\n",
       " 'last': 8,\n",
       " 'chance': 2,\n",
       " 'and': 0,\n",
       " 'if': 6,\n",
       " 'you': 15,\n",
       " 'do': 3,\n",
       " 'not': 10,\n",
       " 'have': 5,\n",
       " 'will': 14,\n",
       " 'never': 9,\n",
       " 'get': 4,\n",
       " 'any': 1,\n",
       " 'one': 11,\n",
       " 'please': 12}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코퍼스에 카운터 벡터 적용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is last chance.',\n",
    "    'and if you do not have this chance.',\n",
    "    'you will never get any chance.',\n",
    "    'will you do get this one?',\n",
    "    'please, get this chance',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0d12c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배열 변환\n",
    "vect.transform(['you will nerver get any chance']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669a92ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last': 6,\n",
       " 'chance': 1,\n",
       " 'if': 5,\n",
       " 'you': 11,\n",
       " 'do': 2,\n",
       " 'not': 8,\n",
       " 'have': 4,\n",
       " 'will': 10,\n",
       " 'never': 7,\n",
       " 'get': 3,\n",
       " 'any': 0,\n",
       " 'one': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 제거한 카운터벡터\n",
    "vect=CountVectorizer(stop_words=['and','is','please','this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1d47d",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "정보 검색론에서 가중치를 구할때 사용되는 알고리즘\n",
    "- TF : 문서 내에서 특정 단어가 출현한 빈도\n",
    "- DF : 한 단어가 전체 문서에서 얼마나 공통적으로 많이 등장하는지 나타낸 값 즉, 특정 단어가 나타난 문서 개수\n",
    "- IDF : DF값의 역수\n",
    "- 스무딩 : 빈도가 0이라면 분모가 0이 되는 상황이 발생하는데 이를 방지하고자 분모에 1을 더해주는 방법\n",
    "\n",
    "- 사용되는 상황 : \n",
    "    - 키워드 검색을 기반으로 하는 검색 엔진\n",
    "    - 중요 키워드 분석\n",
    "    - 검색 엔진에서 검색 결과의 순위 산정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fcd9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도를 위한 3 x 3 행렬을 만들었습니다.\n",
      "[[1.       0.224325 0.      ]\n",
      " [0.224325 1.       0.      ]\n",
      " [0.       0.       1.      ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF를 적용한 후 행렬로 변환\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "doc=['I like machine learning','I love deep learning','I run everyday']\n",
    "tfidf_vectorizer=TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix=tfidf_vectorizer.fit_transform(doc)\n",
    "doc_distance=(tfidf_matrix*tfidf_matrix.T)\n",
    "print('유사도를 위한', str(doc_distance.get_shape()[0]), 'x',str(doc_distance.get_shape()[1]),'행렬을 만들었습니다.')\n",
    "print(doc_distance.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8b108",
   "metadata": {},
   "source": [
    "# 예측 기반 임베딩\n",
    "\n",
    "신경망 구조 혹은 모델을 이용하여 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e750ef7c",
   "metadata": {},
   "source": [
    "## 워드투벡터(Word2Vec)\n",
    "신경망 알고리즘으로 주어진 텍스트에서 텍스트의 각 단어마다 하나씩 일련의 벡터 출력\n",
    "\n",
    "-> 워드투벡터를 사용하면 특정 단어의 동의어를 찾을 수 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c751f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['once',\n",
       "  'upon',\n",
       "  'a',\n",
       "  'time',\n",
       "  'in',\n",
       "  'london',\n",
       "  ',',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'went',\n",
       "  'out',\n",
       "  'to',\n",
       "  'a',\n",
       "  'dinner',\n",
       "  'party',\n",
       "  'leaving',\n",
       "  'their',\n",
       "  'three',\n",
       "  'children',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'at',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['after',\n",
       "  'wendy',\n",
       "  'had',\n",
       "  'tucked',\n",
       "  'her',\n",
       "  'younger',\n",
       "  'brothers',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'to',\n",
       "  'bed',\n",
       "  ',',\n",
       "  'she',\n",
       "  'went',\n",
       "  'to',\n",
       "  'read',\n",
       "  'a',\n",
       "  'book',\n",
       "  '.'],\n",
       " ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'],\n",
       " ['he', 'was', 'flying', '.'],\n",
       " ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'],\n",
       " ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'],\n",
       " ['“', 'hello', '!'],\n",
       " ['who', 'are', 'you', '?'],\n",
       " ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'],\n",
       " ['“', 'my', 'name', 'is', 'peter', 'pan', '.'],\n",
       " ['my',\n",
       "  'shadow',\n",
       "  'wouldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'stock',\n",
       "  'to',\n",
       "  'me.',\n",
       "  '”',\n",
       "  ',',\n",
       "  'he',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['she', 'asked', 'him', 'to', 'come', 'in', '.'],\n",
       " ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'],\n",
       " ['wendy',\n",
       "  'took',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'and',\n",
       "  'sewed',\n",
       "  'it',\n",
       "  'to',\n",
       "  'his',\n",
       "  'shoe',\n",
       "  'tips',\n",
       "  '.'],\n",
       " ['now',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'followed',\n",
       "  'him',\n",
       "  'wherever',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'went',\n",
       "  '!'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'delighted',\n",
       "  'and',\n",
       "  'asked',\n",
       "  'wendy',\n",
       "  '“',\n",
       "  'why',\n",
       "  'don',\n",
       "  '’',\n",
       "  't',\n",
       "  'you',\n",
       "  'come',\n",
       "  'with',\n",
       "  'me',\n",
       "  'to',\n",
       "  'my',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['the', 'neverland', '.'],\n",
       " ['i',\n",
       "  'lived',\n",
       "  'there',\n",
       "  'with',\n",
       "  'my',\n",
       "  'fairy',\n",
       "  'tinker',\n",
       "  'bell.',\n",
       "  '”',\n",
       "  'wendy',\n",
       "  '?'],\n",
       " ['“', 'oh', '!'],\n",
       " ['what', 'a', 'wonderful', 'idea', '!'],\n",
       " ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'],\n",
       " ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['of', 'course', '!'],\n",
       " ['get',\n",
       "  'them',\n",
       "  'we',\n",
       "  'will',\n",
       "  'all',\n",
       "  'fly',\n",
       "  'together.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'replied',\n",
       "  'and',\n",
       "  'so',\n",
       "  'it',\n",
       "  'was',\n",
       "  '.'],\n",
       " ['five',\n",
       "  'little',\n",
       "  'figures',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'of',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'and',\n",
       "  'headed',\n",
       "  'towards',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'they',\n",
       "  'flew',\n",
       "  'over',\n",
       "  'the',\n",
       "  'island',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'told',\n",
       "  'the',\n",
       "  'children',\n",
       "  'more',\n",
       "  'about',\n",
       "  'his',\n",
       "  'homeland',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'who',\n",
       "  'get',\n",
       "  'lost',\n",
       "  'come',\n",
       "  'and',\n",
       "  'stay',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'and',\n",
       "  'me',\n",
       "  ',',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'],\n",
       " ['the',\n",
       "  'mermaids',\n",
       "  'live',\n",
       "  'in',\n",
       "  'the',\n",
       "  'lagoon',\n",
       "  'around',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['and',\n",
       "  'a',\n",
       "  'very',\n",
       "  'mean',\n",
       "  'pirate',\n",
       "  'called',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'keeps',\n",
       "  'troubling',\n",
       "  'everyone',\n",
       "  '.'],\n",
       " ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'],\n",
       " ['so',\n",
       "  'the',\n",
       "  'captain',\n",
       "  'had',\n",
       "  'to',\n",
       "  'put',\n",
       "  'a',\n",
       "  'hook',\n",
       "  'in',\n",
       "  'its',\n",
       "  'place',\n",
       "  '.'],\n",
       " ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'],\n",
       " ['and', 'rightly', 'so', '!'],\n",
       " ['if',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'ever',\n",
       "  'found',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'it',\n",
       "  'will',\n",
       "  'eat',\n",
       "  'up',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'it',\n",
       "  'couldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'eat',\n",
       "  'last',\n",
       "  'time.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['soon', 'they', 'landed', 'on', 'the', 'island', '.'],\n",
       " ['and',\n",
       "  'to',\n",
       "  'the',\n",
       "  'surprise',\n",
       "  'of',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'let',\n",
       "  'them',\n",
       "  'in',\n",
       "  'through',\n",
       "  'a',\n",
       "  'small',\n",
       "  'opening',\n",
       "  'in',\n",
       "  'a',\n",
       "  'tree',\n",
       "  '.'],\n",
       " ['inside',\n",
       "  'the',\n",
       "  'tree',\n",
       "  'was',\n",
       "  'a',\n",
       "  'large',\n",
       "  'room',\n",
       "  'with',\n",
       "  'children',\n",
       "  'inside',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['somewhere',\n",
       "  'huddled',\n",
       "  'by',\n",
       "  'the',\n",
       "  'fire',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corner',\n",
       "  'and',\n",
       "  'somewhere',\n",
       "  'playing',\n",
       "  'amongst',\n",
       "  'themselves',\n",
       "  '.'],\n",
       " ['their',\n",
       "  'faces',\n",
       "  'lit',\n",
       "  'up',\n",
       "  'when',\n",
       "  'they',\n",
       "  'saw',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  ',',\n",
       "  'and',\n",
       "  'their',\n",
       "  'guests',\n",
       "  '.'],\n",
       " ['“', 'hello', 'everyone', '.'],\n",
       " ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['they',\n",
       "  'will',\n",
       "  'be',\n",
       "  'staying',\n",
       "  'with',\n",
       "  'us',\n",
       "  'from',\n",
       "  'now',\n",
       "  'on.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'introduced',\n",
       "  'them',\n",
       "  'to',\n",
       "  'all',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['a', 'few', 'days', 'passed', '.'],\n",
       " ['and', 'they', 'settled', 'into', 'a', 'routine', '.'],\n",
       " ['wendy',\n",
       "  'would',\n",
       "  'take',\n",
       "  'care',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'in',\n",
       "  'the',\n",
       "  'day',\n",
       "  'and',\n",
       "  'would',\n",
       "  'go',\n",
       "  'out',\n",
       "  'with',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brothers',\n",
       "  'in',\n",
       "  'the',\n",
       "  'evening',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'about',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'would',\n",
       "  'cook',\n",
       "  'for',\n",
       "  'them',\n",
       "  'and',\n",
       "  'stitch',\n",
       "  'new',\n",
       "  'clothes',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'even',\n",
       "  'made',\n",
       "  'a',\n",
       "  'lovely',\n",
       "  'new',\n",
       "  'dress',\n",
       "  'for',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  '.'],\n",
       " ['one',\n",
       "  'evening',\n",
       "  ',',\n",
       "  'as',\n",
       "  'they',\n",
       "  'were',\n",
       "  'out',\n",
       "  'exploring',\n",
       "  'the',\n",
       "  'island',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'warned',\n",
       "  'everyone',\n",
       "  'and',\n",
       "  'said',\n",
       "  ',',\n",
       "  '“',\n",
       "  'hide',\n",
       "  '!'],\n",
       " ['hide', '!'],\n",
       " ['pirates', '!'],\n",
       " ['and',\n",
       "  'they',\n",
       "  'have',\n",
       "  'kidnapped',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'princess',\n",
       "  'tiger',\n",
       "  'lily',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'have',\n",
       "  'kept',\n",
       "  'her',\n",
       "  'there',\n",
       "  ',',\n",
       "  'tied',\n",
       "  'up',\n",
       "  'by',\n",
       "  'the',\n",
       "  'rocks',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'was',\n",
       "  'afraid',\n",
       "  'and',\n",
       "  'the',\n",
       "  'princess',\n",
       "  'would',\n",
       "  'drown',\n",
       "  ',',\n",
       "  'is',\n",
       "  'she',\n",
       "  'fell',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  '.'],\n",
       " ['so',\n",
       "  ',',\n",
       "  'in',\n",
       "  'a',\n",
       "  'voice',\n",
       "  'that',\n",
       "  'sounded',\n",
       "  'like',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  ',',\n",
       "  'he',\n",
       "  'shouted',\n",
       "  'instructions',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'guarded',\n",
       "  'her',\n",
       "  ',',\n",
       "  '“',\n",
       "  'you',\n",
       "  'fools',\n",
       "  '!'],\n",
       " ['let', 'her', 'go', 'at', 'once', '!'],\n",
       " ['do',\n",
       "  'it',\n",
       "  'before',\n",
       "  'i',\n",
       "  'come',\n",
       "  'there',\n",
       "  'or',\n",
       "  'else',\n",
       "  'i',\n",
       "  'will',\n",
       "  'throw',\n",
       "  'each',\n",
       "  'one',\n",
       "  'of',\n",
       "  'you',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'got',\n",
       "  'scared',\n",
       "  'and',\n",
       "  'immediately',\n",
       "  'released',\n",
       "  'the',\n",
       "  'princes',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'quickly',\n",
       "  'dived',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  'and',\n",
       "  'swam',\n",
       "  'to',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'of',\n",
       "  'her',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['soon',\n",
       "  'everyone',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'had',\n",
       "  'rescued',\n",
       "  'the',\n",
       "  'princess',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'had',\n",
       "  'tricked',\n",
       "  'his',\n",
       "  'men',\n",
       "  'he',\n",
       "  'was',\n",
       "  'furious',\n",
       "  '.'],\n",
       " ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'],\n",
       " ['that',\n",
       "  'night',\n",
       "  'wendy',\n",
       "  'told',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'that',\n",
       "  'she',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brother',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'go',\n",
       "  'back',\n",
       "  'home',\n",
       "  'since',\n",
       "  'they',\n",
       "  'missed',\n",
       "  'their',\n",
       "  'parents',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'said',\n",
       "  'if',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'could',\n",
       "  'also',\n",
       "  'return',\n",
       "  'to',\n",
       "  'her',\n",
       "  'world',\n",
       "  'they',\n",
       "  'could',\n",
       "  'find',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'home',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'],\n",
       " ['but',\n",
       "  'the',\n",
       "  'sake',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'he',\n",
       "  'agreed',\n",
       "  ',',\n",
       "  'although',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'sadly',\n",
       "  '.'],\n",
       " ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'],\n",
       " ['the',\n",
       "  'next',\n",
       "  'morning',\n",
       "  'all',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'left',\n",
       "  'with',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'on',\n",
       "  'the',\n",
       "  'way',\n",
       "  ',',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'and',\n",
       "  'his',\n",
       "  'men',\n",
       "  'kidnapped',\n",
       "  'all',\n",
       "  'of',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'tied',\n",
       "  'them',\n",
       "  'and',\n",
       "  'kept',\n",
       "  'them',\n",
       "  'on',\n",
       "  'once',\n",
       "  'of',\n",
       "  'his',\n",
       "  'ships',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'peter',\n",
       "  'found',\n",
       "  'out',\n",
       "  'about',\n",
       "  'it',\n",
       "  'he',\n",
       "  'rushed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'ship',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'himself',\n",
       "  'from',\n",
       "  'a',\n",
       "  'tress',\n",
       "  'branch',\n",
       "  'and',\n",
       "  'on',\n",
       "  'to',\n",
       "  'the',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ship',\n",
       "  'where',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'were',\n",
       "  'tied',\n",
       "  'up',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'bravely',\n",
       "  'and',\n",
       "  'threw',\n",
       "  'over',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'him',\n",
       "  '.'],\n",
       " ['quickly',\n",
       "  'he',\n",
       "  'released',\n",
       "  'everyone',\n",
       "  'from',\n",
       "  'their',\n",
       "  'captor',\n",
       "  '’',\n",
       "  's',\n",
       "  'ties',\n",
       "  '.'],\n",
       " ['wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'michael',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'helped',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'where',\n",
       "  'their',\n",
       "  'friends',\n",
       "  'from',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'camp',\n",
       "  'were',\n",
       "  'ready',\n",
       "  'with',\n",
       "  'smaller',\n",
       "  'boats',\n",
       "  'to',\n",
       "  'take',\n",
       "  'them',\n",
       "  'to',\n",
       "  'safety',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'now',\n",
       "  'went',\n",
       "  'looking',\n",
       "  'for',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'let',\n",
       "  'us',\n",
       "  'finished',\n",
       "  'this',\n",
       "  'forever',\n",
       "  'mr.',\n",
       "  'hook',\n",
       "  '”',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'challenged',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'you',\n",
       "  'have',\n",
       "  'caused',\n",
       "  'me',\n",
       "  'enough',\n",
       "  'trouble',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'time',\n",
       "  'that',\n",
       "  'we',\n",
       "  'finished',\n",
       "  'this.',\n",
       "  '”',\n",
       "  'hook',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['with',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'drawn',\n",
       "  ',',\n",
       "  'he',\n",
       "  'raced',\n",
       "  'towards',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  '.'],\n",
       " ['quick',\n",
       "  'on',\n",
       "  'his',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'stepped',\n",
       "  'aside',\n",
       "  'and',\n",
       "  'pushed',\n",
       "  'hook',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'sea',\n",
       "  'where',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'was',\n",
       "  'waiting',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['everyone',\n",
       "  'rejoiced',\n",
       "  'as',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'was',\n",
       "  'out',\n",
       "  'of',\n",
       "  'their',\n",
       "  'lives',\n",
       "  'forever',\n",
       "  '.'],\n",
       " ['everybody', 'headed', 'back', 'to', 'london', '.'],\n",
       " ['mr.', 'and', 'mrs', '.'],\n",
       " ['darling',\n",
       "  'was',\n",
       "  'so',\n",
       "  'happy',\n",
       "  'to',\n",
       "  'see',\n",
       "  'their',\n",
       "  'children',\n",
       "  'and',\n",
       "  'they',\n",
       "  'agreed',\n",
       "  'to',\n",
       "  'adopt',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'even',\n",
       "  'asked',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'to',\n",
       "  'come',\n",
       "  'and',\n",
       "  'live',\n",
       "  'with',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'said',\n",
       "  ',',\n",
       "  'he',\n",
       "  'never',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'grow',\n",
       "  'up',\n",
       "  ',',\n",
       "  'so',\n",
       "  'he',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'will',\n",
       "  'go',\n",
       "  'back',\n",
       "  'to',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  'promised',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'he',\n",
       "  'will',\n",
       "  'visit',\n",
       "  'again',\n",
       "  'sometime',\n",
       "  '!'],\n",
       " ['and',\n",
       "  'he',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'by',\n",
       "  'his',\n",
       "  'side',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 메모리로 로딩하고 토큰화 사용\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sample=open('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/peter.txt','r',encoding='UTF-8')\n",
    "# 피터팬 데이터셋 로딩\n",
    "s=sample.read()\n",
    "\n",
    "f=s.replace('\\n',' ') # 줄바꿈을 공백으로 변환\n",
    "data=[]\n",
    "\n",
    "for i in sent_tokenize(f): # 로딩한 파일을 각 문장마다 반복\n",
    "    temp=[]\n",
    "    for j in word_tokenize(i): # 문장을 각 단어로 토큰화 \n",
    "        temp.append(j.lower()) # 토큰화된 단어를 소문자로 변환하여 temp에 저장\n",
    "    data.append(temp)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e6d22",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "단어를 여러개 나열한 후 이와 관련된 단어를 추정하는 방식 즉 문장에서 등장하는 n개의 단어 열에서 다음에 등장할 단어를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce5e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\anaconda\\envs\\nlp\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from gensim) (1.21.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02d2b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'wendy' - CBOW :  0.07439384\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에 CBOW 적용 후 peter와 wendy의 유사성 확인\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              vector_size = 100, window = 5) # 1\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "                 \"'wendy' - CBOW : \", \n",
    "      model1.wv.similarity('peter', 'wendy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d81baa",
   "metadata": {},
   "source": [
    "- 1 : word2vec의 파라미터\n",
    "    - data(첫번째 인자) : cbow을 적용할 데이터셋\n",
    "    - min_count : 단어에 대해 최소 빈도수 제한(빈도가 적은 단어는 학습하지 않음)\n",
    "    - size : 워드 벡터의 특징 값, 즉 임베딩된 벡터의 차원\n",
    "    - window : 컨텍스트 윈도우 크기\n",
    "    - sg : sg가 0이면 cbow, 1이면 skip-gram을 의미 기본값은 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d04455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'hook' - CBOW :  0.027709894\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'peter' \" +\n",
    "                 \"'hook' - CBOW : \", \n",
    "      model1.wv.similarity('peter', 'hook')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10143a6",
   "metadata": {},
   "source": [
    "## skip-gram\n",
    "\n",
    "CBOW방식과는 반대로 특정한 단어에서 문맥이 될 수 있는 단어를 예측, 즉 중심 단어에서 주변 단어를 예측하는 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed75a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' wendy' - Skip Gram :  0.40088683\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에 skip-gram 적용 후 peter와 wendy 유사성 확인\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, \n",
    "                                             window = 5, sg = 1)\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "          \"wendy' - Skip Gram : \", \n",
    "    model2.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a26f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' hook' - Skip Gram :  0.5201674\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between 'peter' \" +\n",
    "            \"hook' - Skip Gram : \", \n",
    "      model2.wv.similarity('peter', 'hook')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae3340",
   "metadata": {},
   "source": [
    "- CBOW와 skip-gram 중 어떤 알고리즘이 더 좋다고 결론을 내리기보다는 분석하고자 하는 데이터 성격, 분석에 대한 접근 방법 및 도출하고자 하는 결론 등을 종합적으로 고려하여 필요한 라이브러리를 사용할 수 있어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed8613",
   "metadata": {},
   "source": [
    "## 패스트텍스트\n",
    "워드투벡터의 단점을 보완하고자 페이스북에서 개발\n",
    "- 기존 워드투벡터는분산표현을 이용하여 단어의 분산 분포가 유사한 단어들에 비슷한 벡터 값을 할당하여 표현 따라서 워드투벡터는 사전에 없는 단어에 대해서는 벡터 값을 얻을 수 없음\n",
    "- 패스트텍스트는 노이즈에 강하며, 새로운 단어에 대해서는 형태적 유사성을 고려한 벡터값을 얻기 때문에, 자연어 처리 분야에서 많이 사용되는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d7d06",
   "metadata": {},
   "source": [
    "#### 사전에 없는 단어에 벡터 값을 부여하는 방법\n",
    "- 주어진 문서의 각 단어를 n그램으로 표현하는데, n의 설정에 따라 단어의 분리 수준이 결정\n",
    "    - 예를 들어 n을 3으로 결정하면 This is deep learning book을 This is deep, is deep learning, deep learning book으로 분할한 후 임베딩\n",
    "    \n",
    "- 패스트텍스트는 인공 신경망을 이용하여 학습이 완료된 후 데이터셋의 모든 단어를 각  n-그램에 대해 임베딩함. 따라서 사전에 없는 단어가 등장한다면, n-그램으로 분리된 부분 단어와 유사도를 계산하여 의미를 유추할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9de42",
   "metadata": {},
   "source": [
    "#### n-그램\n",
    "n개의 어절/음절을 연쇄적으로 분류하여 그 빈도를 따짐. n이 1일때를 유니그램, 2일때 바이그램, 3일때 트라이그램이라고 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b49bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 및 데이터 호출\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "\n",
    "model=FastText('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/peter.txt',vector_size=4, window=3, min_count=1, epochs=10) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b67064",
   "metadata": {},
   "source": [
    "- 1 : FastText에서 사용하는 파라미터는 Word2Vec와 같음.\n",
    "    - 첫번재 파라미터 : 패스트텍스트를 적용할 데이터셋\n",
    "    - size : 학습할 임베딩의 크기, 즉 임베딩된 벡터의 차원\n",
    "    - window : 고려할 앞뒤 폭(앞뒤 세단어)\n",
    "    - min_count : 단어에 대한 최소 빈도수 제한(1회 이하 단어 무시)\n",
    "    - iter(epochs) : 반복 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039e7b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45924556\n"
     ]
    }
   ],
   "source": [
    "# peter, wendy에 대한 코사인 유사도\n",
    "sim_score=model.wv.similarity('peter','wendy')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467193b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04382521\n"
     ]
    }
   ],
   "source": [
    "# peter, hook에 대한 코사인 유사도\n",
    "sim_score = model.wv.similarity('peter', 'hook')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c5ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리와 사전 훈련된 모델 호출\n",
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_kr = KeyedVectors.load_word2vec_format('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/wiki.ko.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e6ccc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : 노력함, Similarity : 0.80\n",
      "Word : 노력중, Similarity : 0.75\n",
      "Word : 노력만, Similarity : 0.72\n",
      "Word : 노력과, Similarity : 0.71\n",
      "Word : 노력의, Similarity : 0.69\n",
      "Word : 노력가, Similarity : 0.69\n",
      "Word : 노력이나, Similarity : 0.69\n",
      "Word : 노력없이, Similarity : 0.68\n",
      "Word : 노력맨, Similarity : 0.68\n",
      "Word : 노력보다는, Similarity : 0.68\n"
     ]
    }
   ],
   "source": [
    "# 노력과 유사한 단어와 유사도 확인\n",
    "find_similar_to='노력'\n",
    "\n",
    "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
    "    print('Word : {0}, Similarity : {1:.2f}'.format(similar_word[0],similar_word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c1ea31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('초식동물', 0.7804122567176819), ('거대동물', 0.7547270655632019), ('육식동물의', 0.7547166347503662), ('유두동물', 0.753511369228363), ('반추동물', 0.7470758557319641), ('독동물', 0.7466292977333069), ('육상동물', 0.7460315823554993), ('유즐동물', 0.7450903654098511), ('극피동물', 0.7449344396591187), ('복모동물', 0.7424346208572388)]\n"
     ]
    }
   ],
   "source": [
    "# 동물, 육식동물에는 긍정적이지만, 사람에는 부정적인 단어와 유사도 확인\n",
    "similarities=model_kr.most_similar(positive=['동물','육식동물'],negative=['사람'])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48be667c",
   "metadata": {},
   "source": [
    "## 횟수/예측 기반 임베딩\n",
    "## 글로브\n",
    "- 횟수 기반 LSA와 예측 기반 워드투벡터의 단점을 보완하기 위한 모델\n",
    "- 단어에 대한 글로벌 동시 발생 확률 정보를 포함하는 단어 임베딩 방법 즉 단어에 대한 통계 정보와 SKIP-GRAM을 합친 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea362c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리 호출 및 데이터셋 로딩\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath,get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file=datapath('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/glove.6B.100d.txt') # 1\n",
    "word2vec_glove_file=get_tmpfile('glove.6B.100d.word2vec.txt') # 글로브 데이터를 워드투벡터 형태로 변환\n",
    "glove2word2vec(glove_file,word2vec_glove_file) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1e21d",
   "metadata": {},
   "source": [
    "- 1 : 내려받은 glove.6B.100d.txt파일을 메모리로 로딩\n",
    "- 2 : gensim의 glove2word2vec()함수를 사용해 glove를 워드투벡터 형태로 변경 가능. 이후부터는 변경된 형태를 이용하여 기존 워드투벡터의 함수를 사용할 수 있음\n",
    "    - glove_file(첫번째 인자) : 글로브 입력 파일\n",
    "    - 두번째 인자 : 워드투벡터 출력 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ee9c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('legislation', 0.8072139024734497),\n",
       " ('proposal', 0.7306862473487854),\n",
       " ('senate', 0.7142540812492371),\n",
       " ('bills', 0.7044401168823242),\n",
       " ('measure', 0.6958035230636597),\n",
       " ('passed', 0.690624475479126),\n",
       " ('amendment', 0.6846879720687866),\n",
       " ('provision', 0.684556782245636),\n",
       " ('plan', 0.6816462874412537),\n",
       " ('clinton', 0.6663140058517456)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bill과 유사한 단어의 리스트 반환\n",
    "model=KeyedVectors.load_word2vec_format(word2vec_glove_file) # 파이썬 콘솔에서 결과를 확인하기 위해 word2vec_glove_file 파일을 로딩\n",
    "model.most_similar('bill') # 단어(bill) 기준으로 가장 유사한 단어들의 리스트 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9043537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('peach', 0.6888099312782288),\n",
       " ('mango', 0.6838189959526062),\n",
       " ('plum', 0.6684104204177856),\n",
       " ('berry', 0.659035861492157),\n",
       " ('grove', 0.6581552028656006),\n",
       " ('blossom', 0.6503506302833557),\n",
       " ('raspberry', 0.6477391719818115),\n",
       " ('strawberry', 0.6442098617553711),\n",
       " ('pine', 0.6390928626060486),\n",
       " ('almond', 0.6379212737083435)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cherry와 유사한 단어의 리스트 반환\n",
    "model.most_similar('cherry') # 단어 기준으로 가장 유사한 단어들의 리스트 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5aca655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kazushige', 0.48343509435653687),\n",
       " ('askerov', 0.4778185486793518),\n",
       " ('lakpa', 0.46915265917778015),\n",
       " ('ex-gay', 0.45713332295417786),\n",
       " ('tadayoshi', 0.4522107243537903),\n",
       " ('turani', 0.4481006860733032),\n",
       " ('saglam', 0.446959912776947),\n",
       " ('aijun', 0.4435270130634308),\n",
       " ('adjustors', 0.44235295057296753),\n",
       " ('nyum', 0.4423118233680725)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cherry와 관련성 없는 단어의 리스트들 반환\n",
    "model.most_similar(negative=['cherry']) # 단어와 관련성이 없는 단어들을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7248276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen : 0.7699\n"
     ]
    }
   ],
   "source": [
    "# woman, king과 유사성이 높으면서 man과 관련성이 없는 단어를 반환\n",
    "result=model.most_similar(positive=['woman','king'],negative=['man'])\n",
    "print('{} : {:.4f}'.format(*result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bbad33",
   "metadata": {},
   "source": [
    "- 단어간 긍정적, 부정적 관련성을 고려하여 정확하게 결과를 반환하고 있는 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9006a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'champagne'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# australia, beer, france와 관련성이 있는 단어를 반환\n",
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]\n",
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c69d1828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'longest'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tall,tallest,long와 관련성이 있는 단어를 반환\n",
    "analogy('tall','tallest','long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "097cb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# breakfast cereal dinner lunch중 유사도가 낮은 단어를 반환\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split())) # 유사도가 가장 낮은 단어 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d75b68",
   "metadata": {},
   "source": [
    "# 트랜스포머 어텐션\n",
    "- 어텐션 : 언어 번역에서 주로 사용, 인코더와 디코더 네트워크 사용. 즉 입력에 대한 벡터 변환을 인코더에서 처리하고 모든 벡터를 디코더로 보냄. 이렇게 모든 벡터를 전달하는 이유는 기울기 소멸 문제를 해결하기 위해서임. 그렇지만 모든 벡터가 전달되기 때문에, 행렬 크기가 굉장히 커지는 단점이 있는데, 이를 해결하기 위해 소프트맥스함수를 사용해 가중합을 구하고 그 값을 디코더에 전달함\n",
    "\n",
    "- 가중합만 전달되었더라도 정보를 많이 전달받은 디코더는 부담일 수 밖에 없음. 따라서 디코더는 은닉상태에 대해 중점적으로 집중해서 보아야할 벡터를 소프트맥스 함수로 점수를 매긴 후 각각을 은닉 상태의 벡터들과 곱함. 그리고 이 은닉 상태를 모두 더해 하나의 값으로 만듬. 즉, 어텐션은 모든 벡터중에서 꼭 살펴보아야 할 벡터에 집중하는 것\n",
    "\n",
    "- 트랜스포머 : 어텐션을 극대화 하는 방법, 인코더, 디코더를 여러개 중첩시킨 구조, 각각의 인코더, 디코더를 블록이라 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880eb38b",
   "metadata": {},
   "source": [
    "- 인코더 : 단어를 벡터로 임베딩, 이를 셀프 어텐션과 전방향 신경망으로 전달함.\n",
    "    - 셀프 어텐션 : 문장에서 각 단어끼리 얼마나 관계가 있는지 계산해서 반영 즉 문장안에서 단어간의 관계 파악할 수 있음\n",
    "- 디코더 : 3개의 층 셀프 어텐션 층, 인코더-디코더 어텐션, 전방향 신경망 순으로 데이터 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6667a3f",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "- 입력 시퀀스에 대한 출력 시퀀스를 만들기 위한 모델\n",
    "- 번역에 초점을 둔 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6a50aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    " \n",
    "import tensorflow as tf\n",
    "import os \n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2657633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 함수 정의\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    " \n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # 특수문자 제거\n",
    "    w = re.sub(r'[\" \"]+', \" \", w) # 단어와 그 뒤에 오는 구두점 사이에 공백 삽입\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)# a-z, A-Z, ..?..! 제외하고 모두 공백으로 바꿈\n",
    "    w = w.rstrip().strip()# 공백 문자 제거\n",
    "    w = '<start> ' + w + ' <end>'# 문장의 시작과 종료 토큰 생성모델(시작, 끝 시기를 알 수 있도록 지정)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a73fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> May I borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf Puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 확인\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bb47b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ENGLISH, SPANISH] 형태의 단어 변환\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    " \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    " \n",
    "    return zip(*word_pairs)\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    " \n",
    "def tokenize(lang):# 문장의 토큰화\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    " \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    " \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')# padding='post'는 뒤에 0을 채우고 싶을때 사용\n",
    " \n",
    "    return tensor, lang_tokenizer\n",
    " \n",
    "def load_dataset(path, num_examples=None):# 입력, 출력 쌍 만들기(eng/esp)\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    " \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    " \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4b5e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 크기 조정\n",
    "num_examples=30000 # 다양한 세트로 수행하는 것도 좋음\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/spa.txt',num_examples)\n",
    "\n",
    "max_length_targ,max_length_inp=max_length(target_tensor),max_length(input_tensor) # 대상 텐서의 최대 길이 계산\n",
    "\n",
    "input_tensor_train,input_tensor_val,target_tensor_train,target_tensor_val=train_test_split(input_tensor,target_tensor,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35c63583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 초기화\n",
    "BUFFER_SIZE=len(input_tensor_train)\n",
    "BATCH_SIZE=64\n",
    "steps_per_epoch=len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim=256\n",
    "units=1024\n",
    "vocab_inp_size=len(inp_lang.word_index)+1\n",
    "vocab_tar_size=len(targ_lang.word_index)+1\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train)).shuffle(BUFFER_SIZE) # 데이터 분석을 위한 데이터셋 준비\n",
    "dataset=dataset.batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e523c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 네트워크 구축\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform') # 7장에서 배웠던 gru 를 이용한 모델 생성\n",
    " \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    " \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b665340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 구축\n",
    "class EDAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(EDAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1) # 1\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis))) # 2\n",
    " \n",
    "        attention_weights = tf.nn.softmax(score, axis=1) # 어텐션 가중치(attention_weights)의 형태는 (배치크기, 시퀀스 최대 길이, 1)이 됨\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1) # 컨텍스트 벡터의 형태는 (배치 크기, 은닉층 크기)\n",
    "        return context_vector, attention_weights \n",
    "attention_layer = EDAttention(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403405d1",
   "metadata": {},
   "source": [
    "- 1 : tf.expand_dims는 텐서의 원하는 위치에 차원을 추가하는데 사용 즉 query라는 텐서의 1이라는 위치에 차원을 추가한 것으로 은닉층에 하나의 차원을 추가\n",
    "- 2 : score의 형태는 (배치 크기, 시퀀스 최대 길이,1)이 되며, self.W1에 스코어가 적용되기 때문에 마지막 축이 1이 됨. 참고로 self.W1을 적용하기 전 텐서의 형태는 (배치 크기, 시퀀스 최대 길이, 유닛)임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aafc9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 네트워크 구축\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = EDAttention(self.dec_units) # 어텐션 적용\n",
    "\n",
    "    def call(self, x, hidden, enc_output): # 인코더 출력(enc_output) 형태는 (배치크기, 시퀀스 최대 길이, 은닉층 크기)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x) # 임베딩층을 통과한 후 x형태는 (배치크기,1,임베딩차원)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x) # 병합된 벡터를 gru로 보냄\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # 출력 형태는 (배치크기 *1, 은닉층 크기)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33de3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저, 손실함수 정의\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    " \n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1031d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정\n",
    "checkpoint_dir='./training_checkpoints'\n",
    "checkpoint_prefix=os.path.join(checkpoint_dir,'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d02bdfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 함수 정의\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) # 인코더 출력을 디코더로 보냄\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1])) # 손실, 오차 계산\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables)) \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "278c5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6387\n",
      "Epoch 1 Batch 100 Loss 2.2691\n",
      "Epoch 1 Batch 200 Loss 1.8564\n",
      "Epoch 1 Batch 300 Loss 1.6716\n",
      "Epoch 1 Loss 2.0374\n",
      "Epoch 2 Batch 0 Loss 1.7126\n",
      "Epoch 2 Batch 100 Loss 1.5351\n",
      "Epoch 2 Batch 200 Loss 1.3132\n",
      "Epoch 2 Batch 300 Loss 1.1504\n",
      "Epoch 2 Loss 1.3899\n",
      "Epoch 3 Batch 0 Loss 1.0853\n",
      "Epoch 3 Batch 100 Loss 1.1952\n",
      "Epoch 3 Batch 200 Loss 1.0009\n",
      "Epoch 3 Batch 300 Loss 0.8559\n",
      "Epoch 3 Loss 0.9900\n",
      "Epoch 4 Batch 0 Loss 0.7685\n",
      "Epoch 4 Batch 100 Loss 0.6869\n",
      "Epoch 4 Batch 200 Loss 0.6588\n",
      "Epoch 4 Batch 300 Loss 0.6180\n",
      "Epoch 4 Loss 0.6777\n",
      "Epoch 5 Batch 0 Loss 0.5080\n",
      "Epoch 5 Batch 100 Loss 0.4670\n",
      "Epoch 5 Batch 200 Loss 0.5143\n",
      "Epoch 5 Batch 300 Loss 0.4692\n",
      "Epoch 5 Loss 0.4707\n",
      "Epoch 6 Batch 0 Loss 0.3537\n",
      "Epoch 6 Batch 100 Loss 0.2292\n",
      "Epoch 6 Batch 200 Loss 0.3321\n",
      "Epoch 6 Batch 300 Loss 0.3582\n",
      "Epoch 6 Loss 0.3315\n",
      "Epoch 7 Batch 0 Loss 0.2235\n",
      "Epoch 7 Batch 100 Loss 0.3092\n",
      "Epoch 7 Batch 200 Loss 0.2269\n",
      "Epoch 7 Batch 300 Loss 0.2543\n",
      "Epoch 7 Loss 0.2416\n",
      "Epoch 8 Batch 0 Loss 0.1352\n",
      "Epoch 8 Batch 100 Loss 0.1692\n",
      "Epoch 8 Batch 200 Loss 0.1686\n",
      "Epoch 8 Batch 300 Loss 0.2130\n",
      "Epoch 8 Loss 0.1809\n",
      "Epoch 9 Batch 0 Loss 0.0915\n",
      "Epoch 9 Batch 100 Loss 0.1489\n",
      "Epoch 9 Batch 200 Loss 0.1537\n",
      "Epoch 9 Batch 300 Loss 0.1169\n",
      "Epoch 9 Loss 0.1402\n",
      "Epoch 10 Batch 0 Loss 0.0776\n",
      "Epoch 10 Batch 100 Loss 0.1067\n",
      "Epoch 10 Batch 200 Loss 0.1080\n",
      "Epoch 10 Batch 300 Loss 0.1175\n",
      "Epoch 10 Loss 0.1127\n",
      "Time taken for 1 epoch 645.7581868171692 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "EPOCHS = 10\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    " \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    " \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    " \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    " \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3397bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 및 시각화를 위한 함수\n",
    "def evaluate(sentence):\n",
    "    attention_plot=np.zeros((max_length_targ,max_length_inp))\n",
    "    \n",
    "    sentence=preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs=[inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs=tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    " \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, )) # 어텐션 가중치\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0) # 에측 id가 모델에 피드백\n",
    " \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6d9fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 가중치 시각화 함수\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00df9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : <start> esta es mi vida . <end>\n",
      "Predicted translation : this is my life . <end> \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n",
       "    if (this.ResizeObserver === undefined) {\n",
       "        if (window.ResizeObserver !== undefined) {\n",
       "            this.ResizeObserver = window.ResizeObserver;\n",
       "        } else {\n",
       "            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n",
       "            this.ResizeObserver = obs.ResizeObserver;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    this.resizeObserverInstance.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "\n",
       "///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n",
       "// prettier-ignore\n",
       "var _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.on(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    if (event.target !== this) {\n",
       "        // Ignore bubbled events from children.\n",
       "        return;\n",
       "    }\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"999.9999783255842\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 번역을 위한 함수 정의 및 번역 문장 입력 함수\n",
    "def translate(sentence):\n",
    "    result,sentence,attention_plot=evaluate(sentence)\n",
    "    \n",
    "    print('input : %s' % (sentence))\n",
    "    print('Predicted translation : {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')) # 어텐션 가중치 매핑\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)) # 1\n",
    "\n",
    "translate(u'esta es mi vida.') # 스페인어를 영어로 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63557c",
   "metadata": {},
   "source": [
    "- 1 : 체크포인트 디렉터리(checkpoint_dir)에서 최신 체크포인트를 복원하여 문장 번역에 대한 테스트 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ed642",
   "metadata": {},
   "source": [
    "## 버트 (Bert)\n",
    "- 검색 문장의 단어를 입력된 순서대로 하나씩 처리하는 것이 아니라, 한 문장에서 모든 단어의 연관성을 이해하며 검색 문장을 처리하는 모델\n",
    "- 전이 학습 기법에 착안하여 자연어 치레옫 사전에 학습된 신경망을 이용해 목적에 맞게 후처리하는 과정을 거쳐 사용\n",
    "- 전이는 인코더-디코더로 구성, CNN,RNN사용하지 않고 어텐션 모델 도입\n",
    "\n",
    "- 버트의 두가지 버전\n",
    "    - BERT-base(L=12,H=768,A=12) : 학습 파라미터 1.1억개\n",
    "    - BERT-large(L=24,H=1024,A=16) : 학습 파라미터 3.4억개\n",
    "- L은 전이 블록 숫자, H는 은닉층 크기, A는 전이블록에서 사용되는 어텐션 블록 숫자임. 즉 L,H,A가 클수록 블록을 많이 쌓았고, 표현하는 은닉층이 크며, 어텐션 개수를 많이 사용했다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a6b387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (0.14.9)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from bert-for-tf2) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm->params-flow>=0.8.0->bert-for-tf2) (0.4.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\anaconda\\envs\\nlp\\lib\\site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cdbabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리 호출 및 데이터셋 준비\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import pandas as pd\n",
    "import re\n",
    "movie_reviews = pd.read_csv(\"C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/IMDB Dataset.csv\")\n",
    "movie_reviews.isnull().values.any() # 데이터셋에서 어떤 항목이 NaN을 가지고 있는지 확인\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52540b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review' 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리\n",
    "def preprocess_text(sen):\n",
    "    sentence = remove_tags(sen) # html 태그 삭제\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence) # 구두점(punctuation) 및 숫자(number) 제거, 문자가아닌것(a~z,A~Z) 제거\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) # 단일 문자 제거\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence) # 두개이상의 공백 제거\n",
    "    return sentence\n",
    " \n",
    "TAG_RE = re.compile(r'<[^>]+>') # 정규 표현식(<[^>]+>)을 컴파일\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('',text)\n",
    "\n",
    "reviews=[]\n",
    "sentences=list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    reviews.append(preprocess_text(sen)) # 모든 텍스트 리뷰 데이터를 preprocess_text 함수에 적용\n",
    "    \n",
    "print(movie_reviews.columns.values) # 데이터셋의 열에 대한 이름 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e92394e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment열에 대한 고유값 확인\n",
    "movie_reviews.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d55938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정/부정 감정 변환\n",
    "y=movie_reviews['sentiment']\n",
    "y=np.array(list(map(lambda x : 1 if x =='positive' else 0,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136cf111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet \n"
     ]
    }
   ],
   "source": [
    "# 리뷰 출력\n",
    "print(reviews[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e37bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 긍정/부정 리뷰 확인\n",
    "print(y[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1fb85",
   "metadata": {},
   "source": [
    "여기까지 버트를 이용해 모델을 만들 준비가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4328750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트의 토큰화\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer # bert.bert_tokenization 모듈의 FullTokenizer 클래스를 사용하여 객체를 만듬\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False) # 1\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() # 넘파이 배열 형식의 bert 어휘 파일을 만듬\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # 텍스트를 소문자로\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case) # vocabulary_file 및 to_lower_case 변수를 BertTokenizer 객체에 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe16a20",
   "metadata": {},
   "source": [
    "- 1 : hub.KerasLayer에서 버트 모델을 가져온 후 버트 임베딩 레이어를 생성. 이때 매개변수 trainable이 False로 설정되어 있기 때문에, 버트 임베딩은 학습하지 않음. 여기서 주의해야 할 상황은 berttokenizer에만 적용한다는 것 즉 여기서는 버트 임베딩을 사용하지 않고 있다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8495bcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', \"'\", 't', 'be', 'so', 'judgement', '##al']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 문장 토큰화\n",
    "tokenizer.tokenize(\"don't be so judgemental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7ceab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2123, 1005, 1056, 2022, 2061, 16646, 2389]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰의 id 반환\n",
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't be so judgemental\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d244fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 텍스트 데이터 토큰화\n",
    "def tokenize_reviews(text_reviews): \n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews)) # 단일 텍스트 리뷰를 입력으로 받아들이면 토큰화가 된 단어의 id를 반환\n",
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews] # 실제로 입력 데이터셋의 모든 리뷰를 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3690230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
       " array([[ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
       "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,\n",
       "          2305,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128,\n",
       "         15908,  2015,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
       "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
       "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
       "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
       "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
       "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
       "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,\n",
       "          2021,  2196,  2131,  5458,  1997,  3666,  2009,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
       "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
       "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
       "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
       "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
       "             0,     0,     0],\n",
       "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
       "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
       "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
       "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 1037,  7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,\n",
       "          6919,  3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,\n",
       "             0,     0,     0],\n",
       "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
       "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
       "          2033,     0,     0],\n",
       "        [ 1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129,\n",
       "         10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,\n",
       "          2003,     0,     0],\n",
       "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
       "          2115,  3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,\n",
       "          3422,     0,     0],\n",
       "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
       "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
       "          1997,  2068,     0],\n",
       "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
       "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
       "          4276,  3773,     0],\n",
       "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
       "          3522,  2086,  2204, 23191,  5436,  1998, 11813,  6370,  2191,\n",
       "          2023,  2028,  4438],\n",
       "        [ 2307,  3185,  2926,  1996,  2189,  3802,  2696,  2508,  2012,\n",
       "          2197,  2023,  8847,  6702,  2043,  2017,  2031,  2633,  2179,\n",
       "          2008,  2569,  2619],\n",
       "        [ 2023,  3185,  2097,  2467,  2022,  5934,  1998,  3185,  4438,\n",
       "          2004,  2146,  2004,  2045,  2024,  2145,  2111,  2040,  6170,\n",
       "          3153,  1998,  2552]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 0, 1])>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 길이 통일\n",
    "import random\n",
    "\n",
    "reviews_with_len = [[review, y[i], len(review)]\n",
    "                 for i, review in enumerate(tokenized_reviews)] # 토큰화된 리뷰, 리뷰 레이블, 리뷰 길이가 포함된 리스트 생성\n",
    "random.shuffle(reviews_with_len) # 1\n",
    "reviews_with_len.sort(key=lambda x: x[2]) # sort() 메서드를 사용하여 리뷰를 기준으로 데이터 정렬\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32)) # sorted_reviews_labels,output_types에 대한 결과를 int32 형태로 출력\n",
    "BATCH_SIZE = 32 # 배치 크기를 32로 설정, 즉 리뷰 32건을 처리한 후 신경망의 가중치를 업데이트\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ())) # 데이터셋에 패딩 적용\n",
    "next(iter(batched_dataset)) # 첫번째 배치를 출력하고 패딩이 어떻게 적용되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4faf4e",
   "metadata": {},
   "source": [
    "- 1 : csv파일에서 전반부는 긍정, 후반부는 부정 리뷰가 포함되어 있어 긍정과 부정 리뷰가 골고루 사용되도록 무작위로 데이터 섞음\n",
    "\n",
    "\n",
    "- 해당 출력 결과는 리뷰에 대해 첫번재 다섯개, 마지막 다섯개의 패딩을 적용한 결과를 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b590554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 훈련과 검증 세트로 분리\n",
    "import math\n",
    "\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE) # 전체 레코드를 32(배치 크기)로 나누어 줌으로써 전체 배치 크기를 구함\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10 # 데이터의 10%는 검증을 위해 남겨놓음\n",
    "batched_dataset.shuffle(TOTAL_BATCHES) \n",
    "test_data = batched_dataset.take(TEST_BATCHES) # test_data에 데이터를 저장하려고 batched_dataset()객체의 take() 메서드를 사용\n",
    "train_data = batched_dataset.skip(TEST_BATCHES) # 나머지 데이터는 skip() 메서드를 사용하여 훈련을 위해 train_data에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b92cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 생성\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"): \n",
    "        super(TEXT_MODEL, self).__init__(name=name) # 1\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = tf.keras.layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = tf.keras.layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = tf.keras.layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = tf.keras.layers.GlobalMaxPool1D() # 합성곱 신경망 계층 세개가 각각 커널또는 필터값(2,3,4)로 초기화됨\n",
    "        # 이때 원한다면 필터크기를 바꿀수있음\n",
    "        self.dense_1 = tf.keras.layers.Dense(units=dnn_units,activation='relu') # 입력과 출력을 모두 연결해주는 층, 호라성화함수는 렐루사용\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate) # 10%에 대해 드롭아웃적용\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = tf.keras.layers.Dense(units=1,\n",
    "                                                   activation='sigmoid')\n",
    "        else:\n",
    "            self.last_dense=tf.keras.layers.Dense(units=model_output_classes,\n",
    "                                                 activation='softmax') # 2\n",
    "            \n",
    "    def call(self, inputs, training): # 함수를 호출하는 것처럼 클래스의 객체도 호출할 수 있게 만들 수 있는데 이때 필요한 메서드가 __calll__\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) # call() 함수 내에서 각 합성곱 신경망계층의 출력에 전역 최대 풀링 적용\n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3],axis=1) # 합성곱층 세개가 함께 연결되고, 그 출력이 첫번재 신경망에 공급됨\n",
    "        concatenated = self.dense_1(concatenated) # 두번째로 연결된 신경망은 클래스 두 개만 포함하므로 출력 감정을 예측하는데 사용\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated) # 모델의 출력층\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e273d25",
   "metadata": {},
   "source": [
    "- 1 : 클래스 생성자에서 일부 속성을 기본값으로 초기화, 이 값들은 추후 TEXT_MODEL 클래스 객체가 만들어질때 전달\n",
    "- 2 : 출력 클래스가 두개이면, 출력 뉴런을 한개 갖고, 시그모이드 함수를 사용하지만, 그렇지 않다면 출력 뉴런 두개, 소프트맥스 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1147b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 초기화\n",
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a85127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 값을 네트워크에 전달\n",
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab0afc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 141s 100ms/step - loss: 0.3066 - accuracy: 0.8636\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 143s 102ms/step - loss: 0.1294 - accuracy: 0.9536\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 143s 102ms/step - loss: 0.0678 - accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 141s 100ms/step - loss: 0.0338 - accuracy: 0.9878\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 146s 104ms/step - loss: 0.0195 - accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20a44cacdc8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "else:\n",
    "    text_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "text_model.fit(train_data,epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "179af916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 2s 10ms/step - loss: 0.4871 - accuracy: 0.8954\n",
      "[0.4871361553668976, 0.895432710647583]\n"
     ]
    }
   ],
   "source": [
    "# 모델 성능 평가\n",
    "results=text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1eb3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import pandas as pd\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae65d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 메모리로 로딩\n",
    "train_data = pd.read_csv('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/train.csv')\n",
    "test_data = pd.read_csv('C:/Users/이신행/OneDrive/바탕 화면/딥러닝텐서플로교과서_예제파일/chap10/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a329abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 토큰화\n",
    "url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2'\n",
    "bert_layer = hub.KerasLayer(url, trainable=True) # 1\n",
    "\n",
    "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be9c8f",
   "metadata": {},
   "source": [
    "- 1 : 버트 토크나이저에서 사용했던 예제와의 차이는 trainiable=True를 사용했다는 것임. 즉 버트 임베딩을 이용하여 학습을 진행하겠다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc415ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "def bert_encoder(texts, tokenizer, max_len=512):    \n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text) # 입력 데이터를 토큰으로 반환\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] # cls/sep 처리(버트에서 입력 값에 대한 임베딩을 위한 식별자)\n",
    "        pad_len = max_len - len(input_sequence) # 제로 패딩 적용\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8292827",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encoder(train_data, tokenizer, max_len=160)\n",
    "train_labels = train_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbfbefbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "positional_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 1024), (None 335141889   positional_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "def build_model(max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                 dtype=tf.int32, name='positional_ids') # 입력층에서 사용할 input_word_ids 정의\n",
    "    input_segment_ids = tf.keras.layers.Input(shape=(max_len,),  # 입력층에서 사용할 input_segment_ids 정의\n",
    "                                    dtype=tf.int32, name='segment_ids') # 입력층에서 사용할 input_mask 정의\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_len,), \n",
    "                              dtype=tf.int32, name='input_mask')\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, \n",
    "                                                 input_mask, \n",
    "                                                 input_segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(clf_output) # 출력층(완전연결층) 정의\n",
    "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_segment_ids], \n",
    "                        outputs=output) # 1\n",
    "    model.compile(optimizer= RMSprop(lr=2e-6), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "model = build_model(max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701529a",
   "metadata": {},
   "source": [
    "- 1 : input_word_ids, input_mask, input_segment_ids를 입력으로 사용하는 버트계층 생성\n",
    "    - 토큰 id : 버트 토크나이저의 토큰 id\n",
    "    - 마스크 id : 패딩 토큰 구분을 위한 id, 즉 시퀀스마다 동일한 길이를 갖도록 패딩을 붙여줌\n",
    "    - 세그먼트 id : 문장을 구분하는 id, 한 문장의 시퀀스가 0이라면 시퀀스에 문장이 2개 있으면서 두번째 문장이라면 1을 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f60f5f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3461 - accuracy: 1.0000 - val_loss: 0.1868 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2790 - accuracy: 1.0000 - val_loss: 0.1311 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1855 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "train_history=model.fit(train_input,train_labels,\n",
    "                       validation_split=0.2,epochs=3,batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b300f40",
   "metadata": {},
   "source": [
    "- 훈련이 진행될수록 오차(loss,val_loss)값은 줄어들고, 정확도(accuracy,val_accuracy)는 높아지므로 훈련이 잘 되었다고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e7937",
   "metadata": {},
   "source": [
    "# 한국어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7c9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제를 진행할 텍스트 생성\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text=\"\"\"과일 가게에 사과가 많이 진열되어 있다\n",
    "그녀가 나에게 사과한 후, 우리는 친해졌다\n",
    "애플은 사과 모양을 로고로 사용한다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3cb75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  1,  2],\n",
       "       [ 0,  0,  0,  1,  2,  3],\n",
       "       [ 0,  0,  1,  2,  3,  4],\n",
       "       [ 0,  1,  2,  3,  4,  5],\n",
       "       [ 1,  2,  3,  4,  5,  6],\n",
       "       [ 0,  0,  0,  0,  7,  8],\n",
       "       [ 0,  0,  0,  7,  8,  9],\n",
       "       [ 0,  0,  7,  8,  9, 10],\n",
       "       [ 0,  7,  8,  9, 10, 11],\n",
       "       [ 7,  8,  9, 10, 11, 12],\n",
       "       [ 0,  0,  0,  0, 13, 14],\n",
       "       [ 0,  0,  0, 13, 14, 15],\n",
       "       [ 0,  0, 13, 14, 15, 16],\n",
       "       [ 0, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 토큰화\n",
    "tok=Tokenizer()\n",
    "tok.fit_on_texts([text]) # 1\n",
    "\n",
    "vocSize=len(tok.word_index)+1\n",
    "\n",
    "seqs=list()\n",
    "for word in text.split('\\n'):\n",
    "    encoded=tok.texts_to_sequences([word])[0] # 텍스트를 숫자로 반환\n",
    "    for i in range(1,len(encoded)):\n",
    "        seq=encoded[:i+1]\n",
    "        seqs.append(seq)\n",
    "        \n",
    "maxLen=max(len(i) for i in seqs)\n",
    "\n",
    "seqs=pad_sequences(seqs,maxlen=maxLen,padding='pre')\n",
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fce749",
   "metadata": {},
   "source": [
    "- 1 : 단어 단위로 토큰화하여 딕셔너리에 저장, 이때 text를 x에 넣지 않으면 한글자단위로 인코딩됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35dcfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x값에 대한 정의\n",
    "seqs=np.array(seqs) # seqs를 배열로 변환한 후 seqs에 저장\n",
    "x=seqs[:,:-1] # 마지막 열을 제외한 모든 행과 열을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b773ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y값에 대한 정의\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y=seqs[:,-1] # 모든 행과 마지막 열만 취함\n",
    "y=to_categorical(y,num_classes=vocSize) # 케라스에서 제공하는 to_categorical()을 사용하여 원-핫 인코딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c20f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8914 - accuracy: 0.0714\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.8897 - accuracy: 0.0714\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8880 - accuracy: 0.1429\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8863 - accuracy: 0.0714\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8846 - accuracy: 0.0714\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8829 - accuracy: 0.0714\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8812 - accuracy: 0.0714\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8794 - accuracy: 0.0714\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8777 - accuracy: 0.0714\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8759 - accuracy: 0.0714\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8740 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8722 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8702 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8683 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8662 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8641 - accuracy: 0.0714\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8619 - accuracy: 0.0714\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8597 - accuracy: 0.0714\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8573 - accuracy: 0.0714\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8549 - accuracy: 0.0714\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8524 - accuracy: 0.0714\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8497 - accuracy: 0.0714\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8470 - accuracy: 0.0714\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8442 - accuracy: 0.0714\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8412 - accuracy: 0.0714\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8381 - accuracy: 0.0714\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8348 - accuracy: 0.0714\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8314 - accuracy: 0.0714\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8279 - accuracy: 0.0714\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8242 - accuracy: 0.0714\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8203 - accuracy: 0.0714\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8162 - accuracy: 0.0714\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8119 - accuracy: 0.0714\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8074 - accuracy: 0.0714\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8027 - accuracy: 0.1429\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7978 - accuracy: 0.1429\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7925 - accuracy: 0.1429\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7871 - accuracy: 0.1429\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7813 - accuracy: 0.1429\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7753 - accuracy: 0.1429\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7689 - accuracy: 0.1429\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7623 - accuracy: 0.1429\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7552 - accuracy: 0.1429\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7479 - accuracy: 0.1429\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7401 - accuracy: 0.1429\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7320 - accuracy: 0.1429\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7234 - accuracy: 0.1429\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7144 - accuracy: 0.1429\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7050 - accuracy: 0.1429\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6951 - accuracy: 0.1429\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6847 - accuracy: 0.1429\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6739 - accuracy: 0.1429\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6626 - accuracy: 0.1429\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6508 - accuracy: 0.1429\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6385 - accuracy: 0.1429\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6257 - accuracy: 0.1429\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.6124 - accuracy: 0.1429\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5985 - accuracy: 0.1429\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5842 - accuracy: 0.1429\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5694 - accuracy: 0.1429\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.5542 - accuracy: 0.1429\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5384 - accuracy: 0.1429\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5223 - accuracy: 0.1429\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5056 - accuracy: 0.1429\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4886 - accuracy: 0.1429\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4711 - accuracy: 0.2143\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4532 - accuracy: 0.2143\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4349 - accuracy: 0.2143\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4162 - accuracy: 0.3571\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3970 - accuracy: 0.4286\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3774 - accuracy: 0.4286\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3573 - accuracy: 0.3571\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3368 - accuracy: 0.2857\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3159 - accuracy: 0.3571\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2946 - accuracy: 0.3571\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2728 - accuracy: 0.3571\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2508 - accuracy: 0.3571\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2284 - accuracy: 0.3571\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2057 - accuracy: 0.3571\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1828 - accuracy: 0.4286\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1598 - accuracy: 0.4286\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1366 - accuracy: 0.4286\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1134 - accuracy: 0.4286\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0901 - accuracy: 0.4286\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0668 - accuracy: 0.5714\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0436 - accuracy: 0.5714\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0204 - accuracy: 0.5714\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9974 - accuracy: 0.6429\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9745 - accuracy: 0.5714\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9518 - accuracy: 0.5714\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9293 - accuracy: 0.5714\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9070 - accuracy: 0.5714\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8850 - accuracy: 0.5714\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8632 - accuracy: 0.6429\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8416 - accuracy: 0.7143\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8202 - accuracy: 0.7143\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7991 - accuracy: 0.7143\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7782 - accuracy: 0.7143\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7575 - accuracy: 0.7143\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7370 - accuracy: 0.7143\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7167 - accuracy: 0.7143\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6965 - accuracy: 0.7143\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6765 - accuracy: 0.7143\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6566 - accuracy: 0.7143\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6369 - accuracy: 0.7143\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6172 - accuracy: 0.7143\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5977 - accuracy: 0.7143\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5783 - accuracy: 0.7143\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5591 - accuracy: 0.7143\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5399 - accuracy: 0.7143\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5209 - accuracy: 0.7143\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5021 - accuracy: 0.7143\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4834 - accuracy: 0.7857\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4648 - accuracy: 0.8571\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4463 - accuracy: 0.8571\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4279 - accuracy: 0.8571\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.4097 - accuracy: 0.8571\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3916 - accuracy: 0.8571\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3735 - accuracy: 0.8571\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3555 - accuracy: 0.8571\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3376 - accuracy: 0.8571\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.3197 - accuracy: 0.8571\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3019 - accuracy: 0.8571\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2840 - accuracy: 0.8571\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2661 - accuracy: 0.8571\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2483 - accuracy: 0.8571\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2304 - accuracy: 0.8571\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2126 - accuracy: 0.8571\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1948 - accuracy: 0.8571\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1771 - accuracy: 0.8571\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1596 - accuracy: 0.8571\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1422 - accuracy: 0.8571\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1250 - accuracy: 0.8571\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1080 - accuracy: 0.8571\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0912 - accuracy: 0.8571\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0748 - accuracy: 0.8571\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0585 - accuracy: 0.8571\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0425 - accuracy: 0.8571\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0267 - accuracy: 0.8571\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0111 - accuracy: 0.8571\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9958 - accuracy: 0.8571\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9808 - accuracy: 0.8571\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9660 - accuracy: 0.7857\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9516 - accuracy: 0.7857\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9376 - accuracy: 0.7857\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9239 - accuracy: 0.7857\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9105 - accuracy: 0.7857\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8975 - accuracy: 0.7857\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8848 - accuracy: 0.7857\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8725 - accuracy: 0.8571\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8604 - accuracy: 0.9286\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8487 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8372 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8261 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8151 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8044 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7939 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7836 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7736 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7637 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7540 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7444 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7350 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7258 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7168 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7079 - accuracy: 1.0000\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6992 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6824 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6742 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6506 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6430 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6282 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6211 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6140 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6071 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6003 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5871 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5743 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5620 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5559 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5500 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5441 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5383 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5326 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5049 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4892 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15e9c4cb888>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성 및 훈련\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import LSTM \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = Sequential() # 모델 생성\n",
    "model.add(Embedding(vocSize, 10, input_length= maxLen-1, ))         \n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(vocSize, activation=\"softmax\")) # 각 단어의 임베딩 벡터가 10차원\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics = [\"accuracy\"], optimizer =\"adam\")\n",
    "model.fit(x,y,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f31714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 예측\n",
    "def sentGen(model, tok, word, n): # 모델, 토크나이저, 입력단어, 예측 단어 개수를 파라미터로 사용\n",
    "    sent = \"\"\n",
    "    word2=word\n",
    "    for _ in range(n): # 2회 반복\n",
    "        encoded = tok.texts_to_sequences([word])[0] \n",
    "        encoded = pad_sequences([encoded], maxlen = 7, padding=\"pre\")\n",
    "        res=model.predict(encoded)\n",
    "        res=np.argmax(res,axis=1)\n",
    "\n",
    "        for w , i in tok.word_index.items(): \n",
    "            if i == res:  # 예측 단어와 인덱스 단어가 동일할 경우 if문 수행\n",
    "                break \n",
    "        word = word + \" \" + w\n",
    "        sent = sent + \" \" + w\n",
    "    sent = word2 + sent \n",
    "    return sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7b2b8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 7)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13156/901142170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentGen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"과일\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13156/3323918749.py\u001b[0m in \u001b[0;36msentGen\u001b[1;34m(model, tok, word, n)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pre\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\anaconda\\envs\\nlp\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 7)\n"
     ]
    }
   ],
   "source": [
    "print(sentGen(model, tok, \"과일\",2))\n",
    "# 오류가 나서 최종 결과는 올리지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6676f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad5b1646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.7.0\n",
      "  Using cached tensorflow-2.7.0-cp37-cp37m-win_amd64.whl (430.8 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.1.0)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-12.0.0-2-py2.py3-none-win_amd64.whl (13.0 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (0.23.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (3.17.2)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (0.3.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.36.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (3.10.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (0.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.21.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorflow==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.6.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.5.0)\n",
      "Installing collected packages: libclang, keras, flatbuffers, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\anaconda\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\~ensorflow\\\\lite\\\\experimental\\\\microfrontend\\\\python\\\\ops\\\\_audio_microfrontend_op.so'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3ce75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
